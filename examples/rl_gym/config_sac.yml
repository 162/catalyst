args:
  logdir: ./logs/rl_gym  #  change me
  expdir: null

  vis: 0
  infer: 0  #  change me
  train: 1  #  change me

redis:
  port: 12000
  prefix: gym

environment:
  environment: GymWrapper
  env_name: LunarLanderContinuous-v2
  history_len: 3
  frame_skip: 2
  visualize: false
  reward_scale: 0.01
  step_delay: 0.01

algorithm:
  algorithm: SAC

  n_step: 1
  gamma: 0.99

  actor_tau: 0.01
  critic_tau: 0.01
  reward_scale: 200.0
  min_action: &min_action -1.0  # @TODO: take from env
  max_action: &max_action 1.0   # @TODO: take from env
  #critic_distribution: quantile

  actor_optimizer_params:
    optimizer: Adam
    lr: 0.0003
  critic_optimizer_params:
    optimizer: Adam
    lr: 0.0003

  actor_grad_clip_params:
    func: clip_grad_value_
    clip_value: 1.0

  actor_scheduler_params:
    scheduler: MultiStepLR
    milestones: [2000000]  # batches
    gamma: 1.0
  critic_scheduler_params:
    scheduler: MultiStepLR
    milestones: [2000000]  # batches
    gamma: 1.0

  critic_loss_params:
    criterion: HuberLoss
    clip_delta: 1.0

agents:
  actor:
    agent: Actor
    head_hiddens: [128, 128]
    layer_fn: Linear
    bias: false
    norm_fn: LayerNorm
    activation_fn: ReLU
    out_activation: Tanh
    policy_type: gauss

  critic:
    agent: ContinuousCritic
    head_hiddens: [128, 128]
    num_atoms: 1
    layer_fn: Linear
    bias: false
    norm_fn: LayerNorm
    activation_fn: ReLU

trainer:
  batch_size: 256              # transitions
  num_workers: 1
  epoch_len: 1000               # batches

  replay_buffer_size: 1000000  # transitions
  start_learning: 500          # transitions

  save_period: 100             # epochs
  weights_sync_period: 1       # epochs
  target_update_period: 1      # batches

sampler:
  weights_sync_period: 1
  buffer_size: 1100

  valid_seeds: [1, 10, 1000, 10000, 42000]

  exploration:
    - strategy: GaussNoise
      probability: 0.6
      params:
        sigma: 0.2

    - strategy: ParameterSpaceNoise
        probability: 0.3
        params:
          target_sigma: 0.2

    - strategy: Greedy
      probability: 0.1

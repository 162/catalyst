args:
  logdir: ./logs/rl_gym  #  change me
  expdir: null

  vis: 0
  infer: 0  #  change me
  train: 1  #  change me

db:
  port: 12000
  prefix: gym  # TODO: remove

environment:
  environment: GymWrapper
  env_name: LunarLander-v2

  history_len: 3

  frame_skip: 2
  reward_scale: 0.01
  step_delay: 0.01

agents:
  critic:
    agent: ActionCritic

    state_net_params:  # state -> hidden representation
      encoder_net_params:
        hiddens: [128]
        layer_fn: Linear
        norm_fn: LayerNorm
        activation_fn: ReLU
        bias: false
      main_net_params:
        hiddens: [128]
        layer_fn: Linear
        norm_fn: LayerNorm
        activation_fn: ReLU
        bias: false
    value_head_params:  # hidden representation -> ~policy
      in_features: 128
      num_atoms: 1

#      distribution: categorical
#      num_atoms: 4
#      values_range: [-10.0, 10.0]

#      distribution: quantile
#      num_atoms: 4

algorithm:
  algorithm: DQN

  n_step: 1
  gamma: 0.99
  critic_tau: 0.01

  critic_loss_params:
    criterion: HuberLoss
    clip_delta: 10.0

  critic_optimizer_params:
    optimizer: Adam
    lr: 0.0003

  critic_scheduler_params:
    scheduler: MultiStepLR
    milestones: [2000000]  # batches
    gamma: 1.0

trainer:
  batch_size: 32               # transitions
  num_workers: 1
  epoch_len: 1000               # batches

  replay_buffer_size: 1000000  # transitions
  start_learning: 500          # transitions

  save_period: 100             # epochs
  weights_sync_period: 1       # epochs
  target_update_period: 1      # batches

sampler:
  weights_sync_period: 1
  buffer_size: 1100

  valid_seeds: [1, 10, 1000, 10000, 42000]

  exploration_params:
    - exploration: EpsilonGreedy
      probability: 0.98
      eps_init: 1.0
      eps_final: 0.05
      annealing_steps: 20000

    - exploration: Greedy
      probability: 0.02

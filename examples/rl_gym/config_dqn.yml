shared:
  observation_size: 8
  num_actions: &num_actions 4
  discrete_actions: True

  history_len: 3
  n_step: 1
  gamma: 0.99

args:
  logdir: ./logs/rl_gym  #  change me
  algorithm: DQN
  environment: GymWrapper
  vis: 0
  infer: 0  #  change me
  train: 1  #  change me

redis:
  port: 12000
  prefix: gym

env:
  env_name: LunarLander-v2
  env_type: discrete
  reward_scale: 0.01
  frame_skip: 2
  step_delay: 0.01

critic:
  agent: DiscreteCritic
  observation_hiddens: []
  head_hiddens: [128, 128, *num_actions, 64]
  layer_fn: Linear
  bias: false
  norm_fn: LayerNorm
  activation_fn: ReLU

algorithm:
  critic_distribution: categorical
  values_range: [-10.0, 10.0]
  critic_tau: 0.01

  critic_optimizer_params:
    optimizer: Adam
    lr: 0.0003

  critic_scheduler_params:
    scheduler: MultiStepLR
    milestones: [2000000]  # batches
    gamma: 1.0

  critic_loss_params:
    criterion: HuberLoss
    clip_delta: 1.0

trainer:
  batch_size: 256              # transitions
  n_workers: 1
  replay_buffer_size: 1000000  # transitions
  start_learning: 500          # transitions
  epoch_len: 1000               # batches
  target_update_period: 1      # batches
  save_period: 1               # epochs
  weights_sync_period: 1       # epochs

sampler:
  weights_sync_period: 1
  buffer_size: 1100

seeds: [1, 10, 1000, 10000, 42000]

exploration:
  - strategy: EpsilonGreedy
    probability: 0.95
    params:
      eps_init: 1.0
      eps_final: 0.02
      annealing_steps: 100000
      num_actions: *num_actions

  - strategy: Greedy
    probability: 0.05
    params:
